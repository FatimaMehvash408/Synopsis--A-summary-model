{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Major_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SYNOPSIS: A SUMMARY MODEL"
      ],
      "metadata": {
        "id": "O8PLjGmG4Xh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synopsis is a summarization system which takes any input (text/image/audio/video) and generate a summary of the events present in the input. "
      ],
      "metadata": {
        "id": "ke-IlVEaA_DO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Summarization"
      ],
      "metadata": {
        "id": "-WTeWMaD_QyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the necessary libraries:"
      ],
      "metadata": {
        "id": "YNcIcx_t_aek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "from glob import glob\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt \n",
        "import matplotlib.image as mpimg \n",
        "from os import listdir   #listdir helps searching through a given path for all the files in the directory, return list of files in directory\n",
        "from pickle import dump #pickle is used to serialize or deserialize a python object structure, pickle..dump is used to store object data to file\n",
        "# dump() converts a Python object hierarchy into a byte stream\n",
        "from keras.applications.vgg16 import VGG16   #vgg16 model\n",
        "from keras.preprocessing.image import load_img #load_img func is used to load image from file as a pil image\n",
        "#PIL-Python Imaging Library which provides the python interpreter with image editing capabilities\n",
        "from keras.preprocessing.image import img_to_array  #convert pil image instance to a numpy array\n",
        "from keras.applications.vgg16 import preprocess_input   #used to preprocess input image to extract features from it\n",
        "from keras.models import Model  #insctanciate a model to include the necessary layers given some input arrays and tensors and input arrays and tensors\n",
        "import string   #to perform text based operations like getting rid of puntuations from text strings etc\n",
        "from pickle import load   # reads the pickled byte stream of one or more python objects from a file object\n",
        "from numpy import array\n",
        "import tensorflow\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from numpy import argmax\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "srAwE9nN4VIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All functions required:"
      ],
      "metadata": {
        "id": "Y4P4eaEQ_hDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kaggle_initialize(): \n",
        "  #!pip install kaggle - if kaggle is not installed\n",
        "  files.upload()    #upload kaggle.json file \n",
        "  !mkdir -p ~/.kaggle\n",
        "  !cp kaggle.json ~/.kaggle/\n",
        "  !chmod 600 ~/.kaggle/kaggle.json  #Change the permission\n",
        "  !kaggle datasets download -d adityajn105/flickr8k\n",
        "  file_name=\"flickr8k.zip\"    # path of zip file used\n",
        "  with ZipFile(file_name,'r') as zip:\n",
        "    zip.extractall()\n",
        "    print('Done')\n",
        "\n",
        "def plot_images(train_dir):   # use to see random images of the dataset used \n",
        "    plt.figure(figsize = (10,8))   # train_dir = '/content/Images'\n",
        "    image = random.choice(os.listdir(train_dir ))\n",
        "    image_path = train_dir+ '/' + image\n",
        "    img = mpimg.imread(image_path)/255\n",
        "    plt.imshow(img)\n",
        "    plt.axis(False)\n",
        "\n",
        "# extract features from each photo in the directory\n",
        "#contains all steps to extract features from each image\n",
        "#it inputs directory which is declared as the argument to the function\n",
        "def img_feature_extract(path_dir):\n",
        "    img_features = dict()    #empty dictionary created to store image features extracted from each photo\n",
        "    vgg = VGG16()  # load the model using vgg16 class\n",
        "    ''' re-structure the model by removing the last layer from the loaded model\n",
        "  vgg models are used to classify the images and we are not interested in classifying the image therefore we are removing the lastlayer\n",
        "  we are interested in internal representation of image right before classification is made which will be treated as features \n",
        "   the model has extracted from the image'''\n",
        "    vgg.layers.pop()\n",
        "    vgg = Model(inputs=vgg.inputs, outputs=vgg.layers[-1].output)\n",
        "    print(vgg.summary())       #return summary of the model to show the architecture of entire vgg16 model\n",
        "    for img in listdir(path_dir):    # this loop goes through each image in Flicker8k_Dataset\n",
        "        image = img_to_array(load_img(path_dir + '/' + img, target_size=(224, 224)))  #load image as per the target size & convert the image pixels to a numpy array\n",
        "        image = preprocess_input(image.reshape((1, image.shape[0], image.shape[1], image.shape[2])))# reshape data for the model that can be inputted in right format in the vgg & prepare the image for the VGG model\n",
        "        feature = vgg.predict(image, verbose=0)     # get features\n",
        "        img_features[img.split('.')[0]] = feature       # get image id & store feature\n",
        "        print('>%s' % img)  \n",
        "    return img_features\n",
        "\n",
        "# extract features from each photo in the directory\n",
        "def extract_single_img_features(file_path):  \n",
        "    model = VGG16() # load the model\n",
        "    model.layers.pop() # re-structure the model\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "    image = load_img(file_path, target_size=(224, 224))     # load the photo\n",
        "    image = img_to_array(image) # convert the image pixels to a numpy array\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2])) # reshape data for the model\n",
        "    image = preprocess_input(image) # prepare the image for the VGG model\n",
        "    features = model.predict(image, verbose=0) # get features\n",
        "    return features\n",
        "\n",
        "def create_features_pkl():  #function to create pickle file of features    \n",
        "  features = img_feature_extract('/content/Images')  #extract features from all images\n",
        "  print('Extracted Features: %d' % len(features))\n",
        "  dump(features, open('features.pkl', 'wb'))\n",
        "\n",
        "'''loading description of each image in order to clean the text to remove any puntuations, numbers and other things\n",
        "converting image description into vocabulary of words so that the embedding layer of lstm model can understand the word tokens\n",
        "to generate correct captions for images'''\n",
        "\n",
        "# load files into memory (eg-captions document)\n",
        "def doc_loader(file_path):\n",
        "    file = open(file_path, 'r') # open the file(captions.txt) as read-only, this file contains descriptions of all images\n",
        "    text = file.read() # read all text\n",
        "    file.close() # close the file\n",
        "    return text\n",
        "\n",
        "# extract descriptions for images  &returns image identifiers and corresponding descriptions stored in dictionary format\n",
        "def descripts_loader(document_path):\n",
        "    img_caps = dict()\n",
        "    for line in document_path.split('\\n'): #process lines by reading each line from document  while splitting the document by new line character\n",
        "        tokens = line.split() #splitting each line by white spaces\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "        img_id, img_desc = tokens[0], tokens[1:] # take the first token as the image id, the rest as the description\n",
        "        img_id = img_id.split('.')[0] # remove filename from image id\n",
        "        img_desc = ' '.join(img_desc) # convert description tokens back to string\n",
        "        if img_id not in img_caps:# create the list when new images appear\n",
        "            img_caps[img_id] = list()\n",
        "        img_caps[img_id].append(img_desc)# store descriptions in dictionary with key as image identifier\n",
        "    return img_caps\n",
        "\n",
        "# this function is used to clean the text description & takes input dictionary(img_caps) as the argument\n",
        "def descripts_cleaner(descriptions):\n",
        "    trans_table = str.maketrans('', '', string.punctuation) # prepare translation table for removing punctuation\n",
        "    for key, descripts_list in descriptions.items():\n",
        "        for i in range(len(descripts_list)):\n",
        "            descripts = descripts_list[i].split()# tokenize\n",
        "            descripts = [word.lower() for word in descripts] # convert to lower case to maintain consistency\n",
        "            descripts = [w.translate(trans_table) for w in descripts] # remove punctuations from each token\n",
        "            descripts = [word for word in descripts if len(word)>1]# remove single character words (remove hanging 's' and 'a')\n",
        "            descripts = [word for word in descripts if word.isalpha()]  # remove tokens with numbers in them\n",
        "            descripts_list[i] =  ' '.join(descripts)  # store as string\n",
        "\n",
        "# convert the loaded descriptions into a vocabulary of words(a set) and returns it\n",
        "def vocab_add(descripts):\n",
        "    descriptions = set() # build a unique list of all description strings\n",
        "    for key in descripts.keys():\n",
        "# for each image the description is first split by whitespaces and then a set of unique words is formed out of the description\n",
        "        [descriptions.update(d.split()) for d in descripts[key]]\n",
        "    return descriptions\n",
        "\n",
        "#first argument is the mapping dictionary\n",
        "#second argument is the name of the file where you want to store the cleaned descriptionsalong with the unique image identifiers\n",
        "# save descriptions to file, one per line\n",
        "def descripts_saver(descripts, filename):\n",
        "    lines = list()\n",
        "    for key, descripts_list in descripts.items():\n",
        "        for desc in descripts_list:  #apending each file name with a token in description\n",
        "            lines.append(key + ' ' + desc)\n",
        "    file = open(filename, 'w')\n",
        "    file.write('\\n'.join(lines))\n",
        "    file.close()\n",
        "\n",
        "def create_descripts_txt():  ########################################\n",
        "    doc = doc_loader('/content/captions.txt')  # load captions document into memory\n",
        "    descriptions = descripts_loader(doc)  # parse descriptions by extracting descriptions for images\n",
        "    print('Loaded: %d ' % len(descriptions))   \n",
        "    descripts_cleaner(descriptions)  #clean the text description\n",
        "    vocabulary = vocab_add(descriptions) # summarize vocabulary\n",
        "    print('Vocabulary Size: %d' % len(vocabulary))\n",
        "    descripts_saver(descriptions, 'descriptions.txt') # save to file\n",
        "\n",
        "def deleteLineDescripts():  ########################################\n",
        " fn = '/content/descriptions.txt'     #removes fist lines from this txt file\n",
        " f = open(fn)\n",
        " output = []\n",
        " str=\"image\"\n",
        " for line in f:\n",
        "   if not line.startswith(str):\n",
        "    output.append(line)\n",
        " f.close()\n",
        " f = open(fn, 'w')\n",
        " f.writelines(output)\n",
        " f.close()\n",
        "\n",
        "def deleteLineCaptions():  ########################################\n",
        " fn = '/content/captions.txt'     #removes fist lines from this txt file\n",
        " f = open(fn)\n",
        " output = []\n",
        " str=\"image\"\n",
        " for line in f:\n",
        "   if not line.startswith(str):\n",
        "    output.append(line)\n",
        " f.close()\n",
        " f = open(fn, 'w')\n",
        " f.writelines(output)\n",
        " f.close()\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def list_loader(file_path):\n",
        "    document = doc_loader(file_path)  #reading content of the given file\n",
        "    data = list()\n",
        "    for line in document.split('\\n'):# process line by line\n",
        "        if len(line) < 1:# skip empty lines\n",
        "            continue\n",
        "        # identifier = line.split('.')[0]\n",
        "        data.append(line.split('.')[0])# get the image identifier\n",
        "    return list(set(data)) #returning list of unique identifiers\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def clean_descripts_loader(filename, data):\n",
        "    doc = doc_loader(filename) # load document\n",
        "    descripts = dict()\n",
        "    for line in doc.split('\\n'):# split line by white space\n",
        "        tokens = line.split()\n",
        "        img_id, img_desc = tokens[0], tokens[1:] # split id from description\n",
        "        if img_id in data:  # skip images not in the set\n",
        "            if img_id not in descripts: # create list od descriptions\n",
        "                descripts[img_id] = list()\n",
        "            desc = 'startseq ' + ' '.join(img_desc) + ' endseq'  # wrap description in tokens for identification purpose\n",
        "            descripts[img_id].append(desc)  # store in dictionary\n",
        "    return descripts\n",
        "\n",
        "# load photo features , first argument pickle file\n",
        "def img_features_loader(filename, dataset):\n",
        "    all_features = load(open(filename, 'rb'))  # load all features\n",
        "    features = {k: all_features[k] for k in dataset} # filter features\n",
        "    return features\n",
        "''''\n",
        "In this project I have used RNN/LSTM model which is a sequence processor which uses an embedding layer which is the word representation of words\n",
        "with similar meaning to have a similar representation to identify the context of the words. using word embeddings words can be represented in the form \n",
        "of real value vectors. LSTM is used to process the text data which in now in the vector form after passing through embedding layer and helps finds \n",
        "correlationbetween different words. A pre-trained VGG16 model(ouput layer removed) is used to extract features from images. Both these models are merged\n",
        "together and processed by a dense layer in order to predict the captions of an image.\n",
        "\n",
        "'''\n",
        "#encode descriptions into numbers and map them to numeric values for deep learning model to understand the data\n",
        "#creating function to generate sequence of words given image features and encoded text\n",
        "\n",
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def descripts_list(descripts):\n",
        "    descriptions = list()\n",
        "    for key in descripts.keys():\n",
        "        [descriptions.append(d) for d in descripts[key]]\n",
        "    return descriptions\n",
        "\n",
        "# fit a tokenizer given caption descriptions by breaking them into tokens\n",
        "def token_creator(descripts):\n",
        "    descriptions = descripts_list(descripts)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(descriptions)   #helps create individual tokens from the descriptions\n",
        "    return tokenizer\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def descripts_maxLen(descripts):\n",
        "    descriptions = descripts_list(descripts) #generates a list of descriptions from the dictionary inputted\n",
        "    return max(len(d.split()) for d in descriptions)\n",
        "\n",
        "#encoding the text \n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def sequence_creator(tokenizer, max_length, desc_list, image):\n",
        "    img_feat, enc_text, output = list(), list(), list()   #img_feat-stores image features ,enc_text-stores encoded text ,output-this is the output list which stores the next word in the sequence\n",
        "    for descripts in desc_list:  # walk through each description for the image\n",
        "        seq = tokenizer.texts_to_sequences([descripts])[0]  # encode the sequence\n",
        "        for i in range(1, len(seq)):  # split one sequence into multiple X,y pairs\n",
        "            input_seq, output_seq = seq[:i], seq[i]  # split into input and output pair\n",
        "            input_seq = pad_sequences([seq[:i]], maxlen=max_length)[0]  #split into input and output pair and  pad input sequence\n",
        "            output_seq = to_categorical([seq[i]], num_classes=vocab_size)[0]# encode output sequence\n",
        "            img_feat.append(image)  # store the results\n",
        "            enc_text.append(input_seq)\n",
        "            output.append(output_seq)\n",
        "    return array(img_feat), array(enc_text), array(output)\n",
        " \n",
        "#define the captioning model, this function will house the entire architecture of the model used\n",
        "def model_build(vocab_len, descripts_maxLen):\n",
        "\n",
        "    # feature extractor model which uses the pretrained vgg16 model\n",
        "    inputs_1 = Input(shape=(1000,))   #takes input in for of vector with 1000 elements\n",
        "    # input class of keras.layers is being used\n",
        "    feature_extractor_1 = Dropout(0.5)(inputs_1)  # dropout layer use for regularization to reduce overfitting (50% dropout)\n",
        "    feature_extractor_2 = Dense(256, activation='relu')(feature_extractor_1)   #dense layer to process 1000 inputs to output 256 image representation\n",
        "\n",
        "    # sequence model which makes use of rnn/lstm\n",
        "    inputs_2 = Input(shape=(descripts_maxLen,))\n",
        "    sequence_extractor_1 = Embedding(vocab_len, 256, mask_zero=True)(inputs_2)\n",
        "    sequence_extractor_2 = Dropout(0.5)(sequence_extractor_1) # dropout layer use for regularization to reduce overfitting (50% dropout)\n",
        "    sequence_extractor_3 = LSTM(256)(sequence_extractor_2)\n",
        " \n",
        "    # decoder model which merges the above 2 models\n",
        "    decoder_1 = add([feature_extractor_2, sequence_extractor_3])   #merges feature extractor model and sequence model\n",
        "    decoder_2 = Dense(256, activation='relu')(decoder_1)\n",
        "    outputs = Dense(vocab_len, activation='softmax')(decoder_2)\n",
        " \n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs_1, inputs_2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        " \n",
        "    # summarize model\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "#Below code is used to progressively load the batch of data,training progresseviy due to lack of memory\n",
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def feature_data(descriptions, images, tokenizer, descripts_maxLen):\n",
        "    while 1: #goes over each image and incooperates the for loop below on it\n",
        "        for key, desc_list in descriptions.items():\n",
        "            image = images[key][0]      # retrieve the photo feature\n",
        "            in_img, in_seq, out_word = sequence_creator(tokenizer, descripts_maxLen, desc_list, image)\n",
        "            yield [[in_img, in_seq], out_word]\n",
        "\n",
        "def models_20_creator():   #########################################################\n",
        "  # filename = '/content/captions.txt' # load training dataset (6K)\n",
        "  train = list_loader('/content/captions.txt')\n",
        "  train=train[:6000]\n",
        "  train_descripts = clean_descripts_loader('/content/descriptions.txt', train)  # descriptions\n",
        "  train_features = img_features_loader('/content/features.pkl', train) # photo features\n",
        "  tokenizer = token_creator(train_descripts)# prepare tokenizer\n",
        "  vocab_len = len(tokenizer.word_index) + 1\n",
        "  maxLen = descripts_maxLen(train_descripts) # determine the maximum sequence length\n",
        "  img_cap_model = model_build(vocab_len, maxLen)# train the model\n",
        "  steps = len(train_descripts) #6000 mages\n",
        "  for i in range(20):  # train the model, run epochs manually and save after each epoch\n",
        "    generator = feature_data(train_descripts, train_features, tokenizer, maxLen) # create the data generator\n",
        "    img_cap_model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)  # fit for one epoch, supports progressive loading of data\n",
        "    img_cap_model.save('model_' + str(i) + '.h5')  # saving the 20 models seperately\n",
        "\n",
        "# map an integer to a word\n",
        "def word_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "# generate a description for an image\n",
        "def descripts_generator(model, tokenizer, image, max_length):\n",
        "    input_text = 'startseq'     # seed the generation process\n",
        "    for i in range(max_length):     # iterate over the whole length of the sequence\n",
        "        captions_seq = tokenizer.texts_to_sequences([input_text])[0]        # integer encode input sequence\n",
        "        captions_seq = pad_sequences([captions_seq], maxlen=max_length)         # pad input\n",
        "        predicted_output = model.predict([image,captions_seq], verbose=0)       # predict next word\n",
        "        # predicted_output = argmax(predicted_output)       # convert probability to integer\n",
        "        word = word_id(argmax(predicted_output), tokenizer)         # convert probability to integer, map integer to word\n",
        "        if word is None:        # stop if we cannot map the word\n",
        "            break\n",
        "        input_text += ' ' + word        # append as input for generating the next word\n",
        "        if word == 'endseq':        # stop if we predict the end of the sequence\n",
        "            break\n",
        "    return input_text\n",
        "'''\n",
        "BLEU (bilingual evaluation understudy) is an algorithm for\n",
        "evaluating the quality of text which has been machine-translated from one natural language to another.\n",
        "It helps us evaluate how close a generated text is to the expected text. When there can be multiple answers to your input you can use this metric.\n",
        "BLEU results depend strongly on the breadth of your domain, the consistency of the test data with the training and tuning data,\n",
        "and how much data you have available to train. If your models have been trained on a narrow domain, and your training data is\n",
        "consistent with your test data, you can expect a high BLEU score.\n",
        "Here I am using cummulative ngram score to evaluate the model. (1 gram refers to single word, 2 gram means pair of words and so on.)\n",
        "'''\n",
        "# evaluate the skill of the model\n",
        "def model_evaluator(model, descripts, images, tokenizer, max_length):\n",
        "    actual, predicted = list(), list()\n",
        "    for key, desc_list in descripts.items():    # step over the whole set\n",
        "        predicted_output = descripts_generator(model, tokenizer, images[key], max_length)       # generated description stored in yhat\n",
        "        # references = [d.split() for d in desc_list]       # store actual captions in references variable which is then appended to actual list\n",
        "        actual.append([d.split() for d in desc_list])\n",
        "        predicted.append(predicted_output.split())\n",
        "    print('Score->BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))     # calculate BLEU scores\n",
        "    print('Score->BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('Score->BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('Score->BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        " \n",
        "def testset_blue_scores():   ###############################################################\n",
        "  train = list_loader('/content/captions.txt')\n",
        "  train=train[:6000]\n",
        "  train_descriptions = clean_descripts_loader('/content/descriptions.txt', train)\n",
        "  tokenizer = token_creator(train_descriptions)\n",
        "  max_length = descripts_maxLen(train_descriptions)\n",
        "  test = list_loader('/content/captions.txt')\n",
        "  test=test[6000:]\n",
        "  test_descriptions = clean_descripts_loader('/content/descriptions.txt', test)\n",
        "  test_features = img_features_loader('/content/features.pkl', test)\n",
        "  model = load_model('/content/model_19.h5')  # load the model which has minimum loss, in this case it was model_19\n",
        "  evaluate_model(model, test_descriptions, test_features, tokenizer, max_length) # evaluate model\n",
        "\n",
        "#Generate Captions for a Fresh Image\n",
        "def new_img_caption_generator():  ##################################################\n",
        "    tokenizer = load(open('/content/tokenizer.pkl', 'rb')) # load the tokenizer file to retrive the word tokens\n",
        "    max_length = 33 # pre-define the max sequence length (from training)\n",
        "    model = load_model('/content/model_19.h5')# load the model\n",
        "    photo = extract_single_img_features('/content/sample14.jpg') # load and prepare the photograph\n",
        "    description = descripts_generator(model, tokenizer, photo, max_length) # generate description\n",
        "    # query = description\n",
        "    stopwords = ['startseq','endseq']\n",
        "    querywords = description.split()\n",
        "    resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
        "    result = ' '.join(resultwords)\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "2U5KykV44VK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Summarization"
      ],
      "metadata": {
        "id": "Per8wX20ADvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries used:"
      ],
      "metadata": {
        "id": "XxIqQQI_BS9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install newspaper3k  \n",
        "import newspaper # for extracting text from url\n",
        "from newspaper import Article\n",
        "\n",
        "!pip install spacy\n",
        "import en_core_web_sm\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS #import a pre-trained NLP pipeline to help interpret the grammatical structure of the text\n",
        "from string import punctuation\n",
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "BWuoGudhBSbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3173e1a5-6dac-401e-a5ea-d3598aef1eca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 211 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "Collecting feedparser>=5.2.1\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.2\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Collecting jieba3k>=0.35.1\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 32.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Collecting tldextract>=2.0.1\n",
            "  Downloading tldextract-3.3.0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.7.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13553 sha256=54e5538f2dacb3a34054c570c0ee3b705b38a0133f9637ed89f5c8d0f68f4184\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=1a46d63bc514d7dd44d782f091235b2cdd79c0ee83ce41a0c158cfbd8bd49966\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=57ad3cb5e8171d349f386fe590605d805a252e9e9294c71cae593049228156b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=0259d751c8dffece61e0d29372bb4a5e5f77b7007044c0066939500cdaa8288e\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: sgmllib3k, requests-file, tldextract, tinysegmenter, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function used:"
      ],
      "metadata": {
        "id": "LRBe0iTMBaLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Abstractive Text Summarization – \n",
        "#attempts to identify important sections, interpret the context and intelligently generate a summary.\n",
        "\n",
        "# Steps involved:\n",
        "# Look at the use frequency of specific words\n",
        "# Sum the frequencies within each sentence\n",
        "# Rank the sentences based on this sum\n",
        "\n",
        "def summarize(text, per):\n",
        "    nlp = spacy.load('en_core_web_sm')#installing package\n",
        "    doc= nlp(text)\n",
        "    tokens=[token.text for token in doc] #generating tokens\n",
        "    word_frequencies={} #empty dictionary for frequencies\n",
        "    for word in doc: # Counting the number of times a word is used (not including stop words or punctuation)\n",
        "        if word.text.lower() not in list(STOP_WORDS): \n",
        "            if word.text.lower() not in punctuation:\n",
        "                if word.text not in word_frequencies.keys():\n",
        "                    word_frequencies[word.text] = 1 \n",
        "                else:\n",
        "                    word_frequencies[word.text] += 1 # if word is present increase the freq val\n",
        "    max_frequency=max(word_frequencies.values()) # checking which word has the max freq\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word]=word_frequencies[word]/max_frequency  #normalizing the frequencies, more frequently has a higher normalized count.\n",
        "    sentence_tokens= [sent for sent in doc.sents] \n",
        "    sentence_scores = {} #Calculate the sum of the normalized count for each sentence\n",
        "    for sent in sentence_tokens: \n",
        "        for word in sent:\n",
        "            if word.text.lower() in word_frequencies.keys():\n",
        "                if sent not in sentence_scores.keys():                            \n",
        "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
        "                else:\n",
        "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
        "    select_length=int(len(sentence_tokens)*per) #Extracting a percentages \n",
        "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get) #selecting the highest ranked sentences, These serve as our summary\n",
        "    final_summary=[word.text for word in summary] \n",
        "    summary=''.join(final_summary)\n",
        "    return summary "
      ],
      "metadata": {
        "id": "EtMbIJO5AGLF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speech to Text: Videos"
      ],
      "metadata": {
        "id": "uUg_8bRo4R1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and importing the required libraries:"
      ],
      "metadata": {
        "id": "dkPTM6yzF8vT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install SpeechRecognition # Used to recognise and extract the text from the audio file\n",
        "! pip install moviepy # Used to extract the audio from the video file\n",
        "\n",
        "import speech_recognition as sr\n",
        "import moviepy.editor as mp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLcFr5EB_6tY",
        "outputId": "1f8e6e75-b80c-43c0-be4c-82f02d65b813"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 102 kB/s \n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.8.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (0.2.3.5)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.64.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (7.1.2)\n",
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1753088/45929032 bytes (3.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2744320/45929032 bytes (6.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4988928/45929032 bytes (10.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7831552/45929032 bytes (17.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9871360/45929032 bytes (21.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11534336/45929032 bytes (25.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13828096/45929032 bytes (30.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16162816/45929032 bytes (35.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18513920/45929032 bytes (40.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20488192/45929032 bytes (44.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23052288/45929032 bytes (50.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25305088/45929032 bytes (55.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27279360/45929032 bytes (59.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29442048/45929032 bytes (64.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31719424/45929032 bytes (69.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34136064/45929032 bytes (74.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36405248/45929032 bytes (79.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38682624/45929032 bytes (84.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40845312/45929032 bytes (88.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43163648/45929032 bytes (94.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45547520/45929032 bytes (99.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function used:"
      ],
      "metadata": {
        "id": "e1RklJ24AoL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def video_extract(path):\n",
        "\n",
        "  # Loading and storing the video file into a variable\n",
        "  clip = mp.VideoFileClip(path)\n",
        "\n",
        "  # Extracting the audio from the video file and storing it in a 'wav' format audio file named 'converted'\n",
        "  clip.audio.write_audiofile('/content/converted.wav')\n",
        "\n",
        "  # Creating the recognizer object which is used to recognise audios\n",
        "  r = sr.Recognizer()\n",
        "\n",
        "  # Storing the audio file in a variable, 'audio'\n",
        "  audio = sr.AudioFile(\"/content/converted.wav\")\n",
        "\n",
        "  # Iterating through the audio, recognizing it using the Google Speech Recognition API and storing it\n",
        "  # in a text format into the variable, 'result'\n",
        "  with audio as source:\n",
        "    audio_file = r.record(source)\n",
        "  result = r.recognize_google(audio_file)\n",
        "\n",
        "  # returning the extracted text\n",
        "  return(result)"
      ],
      "metadata": {
        "id": "2iN5Jm_c_G80"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speech to Text: Audios"
      ],
      "metadata": {
        "id": "W5vLhOWmGRx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function used:"
      ],
      "metadata": {
        "id": "grGP5oHaGa8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def audio_extract(path):\n",
        "\n",
        "  # Loading and storing the audio file into a variable\n",
        "  clip = mp.AudioFileClip(path)\n",
        "\n",
        "  # Converting the audio format to 'wav' as the speech recognition model works best with that and storing\n",
        "  # it in a file, named 'converted'\n",
        "  clip.write_audiofile('/content/converted1.wav')\n",
        "\n",
        "  # Creating the recognizer object which is used to recognise audios\n",
        "  r = sr.Recognizer()\n",
        "\n",
        "  # Storing the audio file in a variable, 'audio'\n",
        "  audio = sr.AudioFile('/content/converted1.wav')\n",
        "\n",
        "  # Iterating through the audio, recognizing it using the Google Speech Recognition API and storing it\n",
        "  # in a text format into the variable, 'result'\n",
        "  with audio as source:\n",
        "    audio_file = r.record(source)\n",
        "  result = r.recognize_google(audio_file)\n",
        "\n",
        "  # returning the extracted text\n",
        "  return(result)"
      ],
      "metadata": {
        "id": "9HfAy6ffGXHt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Function:"
      ],
      "metadata": {
        "id": "yBypPYhVBiv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "  print('Enter what you want to summarize:')\n",
        "  print('1. Text')\n",
        "  print('2. Image')\n",
        "  print('3. Audio')\n",
        "  print('4. Video')\n",
        "  choice = int(input('Your choice:'))\n",
        "\n",
        "  # Text\n",
        "  if choice == 1:\n",
        "    print('option 1: Enter the text')\n",
        "    print('option 2: Enter the url')\n",
        "    a=int(input('Selected option '))\n",
        "\n",
        "    if a==1:\n",
        "      txt=input('Enter the text')\n",
        "      print('ORIGINAL TEXT')\n",
        "      print(txt)\n",
        "      print('SUMMARIZED TEXT')\n",
        "      print(summarize(txt,0.4))\n",
        "\n",
        "    elif a==2:\n",
        "        url= input('Enter the url ')\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse() #download and parse the article to extract the relevant attributes\n",
        "        #print('ORIGINAL TEXT')\n",
        "        #print( article.text)\n",
        "        print('SUMMARIZED TEXT')\n",
        "        print(summarize(article.text, 0.15))\n",
        "\n",
        "  # Image\n",
        "  elif choice == 2:\n",
        "    new_img_caption_generator()\n",
        "\n",
        "  # Audio\n",
        "  elif choice == 3:\n",
        "    audio_input = input(\"Enter the audio file: \")\n",
        "    text = audio_extract(audio_input)\n",
        "    print(summarize(text,0.4))\n",
        "\n",
        "  # Video\n",
        "  elif choice == 4:\n",
        "    video_input = input(\"Enter the file: \")\n",
        "    text = video_extract(video_input)\n",
        "    print(summarize(text,0.4))"
      ],
      "metadata": {
        "id": "OSeZtebP6cnB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "dh3IL_De_ESJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6846c4c4-2573-47db-8c39-3b29ae4bab21"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter what you want to summarize:\n",
            "1. Text\n",
            "2. Image\n",
            "3. Audio\n",
            "4. Video\n",
            "Your choice:4\n",
            "Enter the file: /content/videoplayback (3).mp4\n",
            "[MoviePy] Writing audio in /content/converted.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 615/615 [00:00<00:00, 1887.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MoviePy] Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm taking one other class online and one class at the Bedford campus is actually the first class I've ever had to make videos to Post onlineI'm in your interview speech class with you all along with this class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit Implementation"
      ],
      "metadata": {
        "id": "UEIRLb6cA2_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''! pip install streamlit\n",
        "import streamlit as st\n",
        "st.write(text)\n",
       
        "#! streamlit run /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py'''"
      ],
      "metadata": {
        "id": "NowTlHSAA1wi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
